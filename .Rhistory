tweets.df <- twListToDF(tweets)
tweets <- userTimeline("RDataMining", n = 3200)
length(tweets)
tweets[1:5]
tweets.df <- twListToDF(tweets)
tweets.df
myCorpus <- tm_map(myCorpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
myCorpus <- tm_map(myCorpus,content_transformer(tolower))
myCorpus <- tm_map(myCorpus,content_transformer(tolower))
myCorpus <- tm_map(myCorpus,removePunctuation)
myCorpus <- tm_map(myCorpus,removeNumbers)
# removing URL
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
#making entra copy of the corpus
myCorpusCopy <- myCorpus
#stem World
# install.packages("SnowballC")
# library(SnowballC)
myCorpus <- tm_map(myCorpus,stemDocument)
#print first 5 lines from the corpus
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
myCorpus <- tm_map(myCorpus,stemCompletion,dictionary = myCorpusCopy,lazy = TRUE)
miningCases <- tm_map(myCorpusCopy,grep,pattern = "\\<mining")
sum(unlist(miningCases))
minerCases <- tm_map(myCorpusCopy,grep,pattern = "\\<miners")
sum(unlist(minerCases))
#replace miners with minings
myCorpus <- tm_map(myCorpus,gsub, pattern = "miners", replacement = "mining",lazy = TRUE)
tdm <- TermDocumentMatrix(myCorpus,control = list(wordLength = c(1, Inf)))
tweets <- userTimeline("RDataMining", n = 3200)
length(tweets)
tweets.df <- twListToDF(tweets)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
myCorpus <- tm_map(myCorpus,content_transformer(tolower))
myCorpus <- tm_map(myCorpus,content_transformer(removePunctuation))
myCorpus <- tm_map(myCorpus,content_transformer(removeNumbers))
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,content_transformer(myURL))
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,content_transformer(myStopwords))
myCorpus <- tm_map(myCorpus,content_transformer(removeWords),myStopwords)
myCorpus <- tm_map(myCorpus,removeWords,myStopwords,lazy = TRUE)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus,content_transformer(stemDocument))
myCorpus <- tm_map(myCorpus,content_transformer(stemDocument),lazy = TRUE)
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
tweets <- userTimeline("RDataMining", n = 3200)
length(tweets)
tweets[1:5]
#Text Cleaning
tweets.df <- twListToDF(tweets)
tweets.df
# library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
myCorpus <- tm_map(myCorpus,content_transformer(tolower))
myCorpus <- tm_map(myCorpus,content_transformer(removePunctuation))
myCorpus <- tm_map(myCorpus,content_transformer(removeNumbers))
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,content_transformer(myURL))
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,content_transformer(myStopwords),lazy = TRUE)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus,content_transformer(stemDocument),lazy = TRUE)
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
getwd()
install.packages("/Users/mavezsinghdabas/tm_0.5-10.tgz", repos = NULL, type="source")
tweets <- userTimeline("RDataMining", n = 3200)
length(tweets)
tweets.df <- twListToDF(tweets)
myCorpus <- Corpus(VectorSource(tweets.df$text))
myCorpus <- tm_map(myCorpus,tolower)
myCorpus <- tm_map(myCorpus,removePunctuation)
myCorpus <- tm_map(myCorpus,removeNumbers)
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
#making entra copy of the corpus
myCorpusCopy <- myCorpus
#stem World
# install.packages("SnowballC")
# library(SnowballC)
myCorpus <- tm_map(myCorpus,stemDocument)
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
myCorpus <- tm_map(myCorpus,stemCompletion,dictionary = myCorpusCopy)
myCorpus <- tm_map(myCorpus,stemCompletion,dictionary = myCorpusCopy,lazy = TRUE)
miningCases <- tm_map(myCorpusCopy,grep,pattern = "\\<mining")
sum(unlist(miningCases))
minerCases <- tm_map(myCorpusCopy,grep,pattern = "\\<miners")
sum(unlist(minerCases))
myCorpus <- tm_map(myCorpus,gsub, pattern = "miners", replacement = "mining",lazy = TRUE)
tdm <- TermDocumentMatrix(myCorpus,control = list(wordLength = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus,control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus,control = list(wordLength = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
tdm <- TermDocumentMatrix(myCorpus)
myCorpus
tweets <- userTimeline("RDataMining", n = 500)
length(tweets)
tweets[1:5]
tweets.df <- twListToDF(tweets)
myCorpus <- Corpus(VectorSource(tweets.df$text))
#myCorpus <- tm_map(myCorpus,content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),mc.cores=1)
myCorpus <- tm_map(myCorpus,tolower)
myCorpus <- tm_map(myCorpus,removePunctuation)
myCorpus <- tm_map(myCorpus,removeNumbers)
# removing URL
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
#making entra copy of the corpus
myCorpusCopy <- myCorpus
#stem World
# install.packages("SnowballC")
# library(SnowballC)
myCorpus <- tm_map(myCorpus,stemDocument)
#print first 5 lines from the corpus
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
myCorpus <- tm_map(myCorpus,stemCompletion,dictionary = myCorpusCopy,lazy = TRUE)
#count frequencies of minings and replacing that with minors
miningCases <- tm_map(myCorpusCopy,grep,pattern = "\\<mining")
sum(unlist(miningCases))
minerCases <- tm_map(myCorpusCopy,grep,pattern = "\\<miners")
sum(unlist(minerCases))
#replace miners with minings
myCorpus <- tm_map(myCorpus,gsub, pattern = "miners", replacement = "mining",lazy = TRUE)
tdm <- TermDocumentMatrix(myCorpus)
tdm <- TermDocumentMatrix(myCorpus,
control = list(wordLengths = c(1, Inf)))
library(SnowballC)
tweets <- userTimeline("RDataMining", n = 500)
length(tweets)
tweets[1:5]
#Text Cleaning
tweets.df <- twListToDF(tweets)
tweets.df
# library(tm)
myCorpus <- Corpus(VectorSource(tweets.df$text))
tdm <- TermDocumentMatrix(myCorpus,
control = list(removePunctuation = TRUE,
stopwords =  TRUE,
removeNumbers = TRUE, tolower = TRUE))
myURL <- function(x) gsub("http[[:alnum:]]*","",x)
myCorpus <- tm_map(myCorpus,myURL)
myStopwords <- c(stopwords("english"),"available","via")
myStopwords <- setdiff(myStopwords, c("r","big"))
myCorpus <- tm_map(myCorpus,removeWords,myStopwords)
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus,stemDocument)
for(i in 1:5){
cat(paste("[[",i,"]]", sep = ""))
writeLines(myCorpus[[i]])
}
tdm
idx <- which(dimnames(tdm)$Terms == "r")
idx
inspect(tdm[idx + (0:5), 101:110])
(freq.terms <- findFreqTerms(tdm, lowfreq = 15))
term.freq <- rowSums(as.matrix(tdm))
term.freq
term.freq <- subset(term.freq, term.freq >= 15)
df <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
install.packages("ggplot2")
install.packages("ggplot2")
ggplot(df, aes(x = term, y = freq)) + geom_bar(stat = "identity") + xlab("Terms") + ylab("Count") + coord_flip()
findAssocs(tdm, "r",0.2)
findAssocs(tdm, "m",0.2)
tdm
findAssocs(tdm, "a",0.2)
findAssocs(tdm, "mining", 0.25)
library(graph)
library(Rgraphviz)
install.packages("Rgraphviz")
install.packages("Rgraphviz")
install.packages("Rgraphviz")
install.packages("Rgraphviz")
library(Rgraphviz)
plot(tdm, term = freq.terms, corThreshold = 0.12, weighting = T)
install.packages("worldcloud")
install.packages("worldcloud")
install.packages("wordcloud")
library(wordcloud)
library(wordcloud)
m <- as.matrix(tdm)
m
word.freq <- sort(rowSums(m), decreasing = T)
word.freq
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,random.order = F)
tdm2 <- removeSparseTerms(tdm, sparse = 0.95) m2 <- as.matrix(tdm2)
tdm2 <- removeSparseTerms(tdm, sparse = 0.95)
m2 <- as.matrix(tdm2)
distMatrix <- dist(scale(m2))
fit <- hclust(distMatrix, method = "ward")
fit <- hclust(distMatrix, method = "ward.D")
plot(fit)
rect.hclust(fit, k = 6)
m3 <- t(m2) # transpose the matrix to cluster documents (tweets)
set.seed(122) # set a fixed random seed
k <- 6 # number of clusters
kmeansResult <- kmeans(m3, k)
round(kmeansResult$centers, digits = 3) # cluster centers
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T) cat(names(s)[1:5], "\n")
# print the tweets of every cluster
for (i in 1:k) {
cat(paste("cluster ", i, ": ", sep = ""))
s <- sort(kmeansResult$centers[i, ], decreasing = T)
cat(names(s)[1:5], "\n")
# print the tweets of every cluster
# print(tweets[which(kmeansResultÂ£cluster==i)])
}
library(flc)
library(fpc)
install.packages("fpc")
library(fpc)
# partitioning around medoids with estimation of number of clusters pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc # number of clusters identified
pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc
pamResult <- pamResult$pamobject
for (i in 1:k) {
cat("cluster", i, ": ", colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n
}
)
)
stop
""
pamResult <- pamk(m3, metric="manhattan")
pamResult <- pamk(m3, metric="manhattan")
k <- pamResult$nc
pamResult <- pamResult$pamobject
for (i in 1:k) {
cat("cluster", i, ": ", colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n"
}
for (i in 1:k) {
cat("cluster", i, ": ",
colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)]
}
for (i in 1:k) {
cat("cluster", i, ": ",
colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], "\n")
}
layout(matrix(c(1, 2), 1, 2)) # set to two graphs per page
plot(pamResult, col.p = pamResult$clustering)
layout(matrix(1))
plot(pamResult, col.p = pamResult$clustering)
dtm <- as.DocumentTermMatrix(tdm)
library(topicmodels)
install.packages("topicmodels")
install.packages("topicmodels")
library(topicmodels)
lda <- LDA(dtm, k = 8) # find 8 topics
term <- terms(lda, 4) # first 4 terms of every topic term
term
term <- apply(term, MARGIN = 2, paste, collapse = ", ")
term
topic <- topics(lda, 1)
topics <- data.frame(date=as.IDate(tweets.df$created), topic)
topic <- data.frame(date=as.IDate(tweets.df$created), topic)
install.packages("saccades")
library(saccades)
library(saccades)
data("samples")
sample()
samples
names(samples)
head(samples)
mavez <- detect.fixations(samples)
mavez
names(mavez)
summary(mavez)
?samples
head(mavez[c(2,3)])
?saccades
calculate.summary(mavez)
?calculate.summary
head(mavez[c(2,3)])
head(samples)
setwd("~/StatisticsForBigData-")
getwd()
read.table("ID_001_1.txt")
read.table(ID_001_1.txt,header = TRUE,)
read.table("ID_001_1.txt",header = TRUE)
data.frame("ID_001_1.txt",header = TRUE)
data.frame("ID_001_1.txt",header = TRUE)
read.csv("ID_001_1.txt")
read.csv("ID_001_1.txt",header = TRUE,sep = ",")
ver1 <- read.csv("ID_001_1.txt",header = TRUE,sep = ",")
head(ver1)
names(ver1)
ver1 <- read.csv("ID_001_1.txt",header = TRUE)
head(ver1)
colnames(ver1)
read.table("ID_001_1.txt")
read.table(file = "ID_001_1.txt",header = TRUE,sep = "\t",colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),fill = FALSE)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
strip.white = TRUE,
)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
strip.white = TRUE
)
getwd()
read.table("/Users/mavezsinghdabas/StatisticsForBigData-/ID_001_1.txt")
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
strip.white = TRUE
)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
#strip.white = TRUE
)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
#fill = FALSE,
#strip.white = TRUE
)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = TRUE,
#strip.white = TRUE
)
read.table(file = "ID_001_1.txt",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = TRUE,
strip.white = TRUE
)
read.table(file = "ID_001_1",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = TRUE,
strip.white = TRUE
)
read.csv("ID_001_1.csv")
ver2 <- read.csv("ID_001_1.csv")
head(ver2)
read.table(file = "ID_001_1",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
strip.white = TRUE
)
read.table(file = "ID_001_1.csv",
header = TRUE,
sep = "\t",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS"),
fill = FALSE,
strip.white = TRUE
)
ver1 <- read.csv("ID_001_1.csv",file = FALSE)
ver1 <- read.csv("ID_001_1.csv",header = TRUE,sep = " ")
ver1 <- read.csv("ID_001_1.csv",header = TRUE,sep = " ",fill = FALSE)
ver1 <- read.csv("ID_001_1.csv",header = TRUE,sep = " ",fill = FALSE)
ver1 <- read.csv("ID_001_1.csv",header = TRUE,fill = FALSE)
ver1
head(ver1)
ver2 <- ver1(c[1:4,])
ver1 <- read.csv("ID_001_1.csv",header = TRUE,fill = FALSE)
read.delim("ID_001_1.txt",header = TRUE)
head(read.delim("ID_001_1.txt",header = TRUE))
rm(ver2)
ver1$SAMPLE
head(ver1$SAMPLE)
head(ver1$X.DEGREE)
head(ver1$VALIDITY)
head(ver1$X.STIMULUS)
head(ver1$Y.STIMULUS)
head(ver1$SAMPLE)
ver1 <- read.csv("ID_001_1.txt",header = TRUE,fill = FALSE)
ver1
head(ver1)
head(ver1$SAMPLE)
head(ver1$X.DEGREE)
head(ver1$Y.DEGREE)
head(ver1$VALIDITY)
head(ver1$X.STIMULUS)
head(ver1)
colnames(ver1) <- c("Xi")
head(ver1)
colnames(ver1) <- c("Xi","Yi")
as.matrix(ver1)
ver.matrix <- as.matrix(ver1)
structure(ver.matrix)
head(ver.matrix)
row.names(ver.matrix)
dim(ver.matrix)
ls()
read.csv(file = "ID_001_1.csv",
header = FALSE,
sep = "",
colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS")
)
read.csv(file = "ID_001_1.csv",
header = FALSE,
sep = ""
# colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS")
)
eye1 <- read.csv(file = "ID_001_1.csv",
header = FALSE,
sep = ""
# colClasses = c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS")
)
head(eye1)
colnames(eye1) <- c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS")
head(eye1)
eye1 <- eye1[-1,]
head(eye1)
dim(eye1)
eye2 <- eye1[c(-5,-6,-7,-8,-9,-10)]
head(eye2)
eye1 <- read.csv(file = "ID_001_1.csv",
header = FALSE,
sep = ""
)
head(eye1)
colnames(eye1) <- c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY","X.STIMULUS","Y.STIMULUS")
head(eye1)
eye1 <- eye1[-1,]
head(eye1)
eye2 <- eye1[c(-5,-6,-7,-8,-9,-10)]
head(eye2)
dim(eye2)
eye3 <- eye2
dim(eye3)
head(eye3)
rm(eye3)
eye.txt <- read.table(file = "ID_001_1.txt",
header = FALSE,
sep = ""
)
eye.txt <- read.table(file = "ID_001_1.txt",
# header = FALSE,
# sep = ""
)
eye.txt <- read.table("ID_001_1.txt",
# header = FALSE,
# sep = ""
)
eye.txt <- read.table("ID_001_1.txt"
# header = FALSE,
# sep = ""
)
eye.txt <- read.table("ID_001_1.txt",
header = TRUE,
fill = TRUE
# sep = ""
)
eye.txt
head(eye.txt)
dim(eye.txt)
eye.txt.2 <- eye.txt[c(-5,-6,-7,-8,-9,-10)]
dim(eye.txt.2)
dim(eye.txt.2)
head(eye.txt.2)
colnames(eye.txt.2)
colnames(eye.txt.2) <- c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY"))
colnames(eye.txt.2) <- c("SAMPLE","X.DEGREE","Y.DEGREE","VALIDITY")
head(eye.txt.2)
dim(eye.txt.2)
sum(eye.txt.2$VALIDITY)
